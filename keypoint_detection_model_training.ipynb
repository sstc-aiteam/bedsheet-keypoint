{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba71d1d1",
   "metadata": {},
   "source": [
    "# 關鍵點偵測模型訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3773dc9e",
   "metadata": {},
   "source": [
    "## 載入資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d056b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "image_data_dir = \"cloth_data_gen/output/images\"\n",
    "keypoint_data_dir = \"cloth_data_gen/output/keypoints\"\n",
    "\n",
    "img_arr = []\n",
    "keypoints_img_arr = []\n",
    "for img_file in os.listdir(image_data_dir):\n",
    "    if img_file.endswith('.png'):\n",
    "        name = img_file.split('.')[0]\n",
    "        keypoint_file = os.path.join(keypoint_data_dir, name + '.txt')\n",
    "        image_path = os.path.join(image_data_dir, img_file)\n",
    "        img = cv2.imread(image_path)\n",
    "        keypoints = pd.read_csv(keypoint_file)\n",
    "        pixels_coords = keypoints[['x_pixel', 'y_pixel']].values\n",
    "        kimg = np.zeros((img.shape[0], img.shape[1]), dtype=np.uint8)\n",
    "        karr = []\n",
    "        # check if all pixels coordinates are within the image bounds\n",
    "        if pixels_coords.shape[0] > 0 and np.all((pixels_coords[:, 0] >= 0) & (pixels_coords[:, 0] < img.shape[1]) & \n",
    "                                                  (pixels_coords[:, 1] >= 0) & (pixels_coords[:, 1] < img.shape[0])):\n",
    "            for point in pixels_coords:\n",
    "                kimg[int(point[1]), int(point[0])] = 255\n",
    "                karr.append([int(point[0]), int(point[1])])\n",
    "            # keypoints_img_arr.append(kimg)\n",
    "            keypoints_img_arr.append(karr)\n",
    "            img_arr.append(img)\n",
    "img_arr = np.array(img_arr)\n",
    "keypoints_img_arr = np.array(keypoints_img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a4188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, images, keypoints, transform=None):\n",
    "        self.images = images.astype(np.float32)\n",
    "        self.keypoints = keypoints.astype(np.float32)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]  # shape (400, 400, 3)\n",
    "        kp = self.keypoints[idx]  # shape (4, 2)\n",
    "        img = np.transpose(img, (2, 0, 1))  # channels first\n",
    "        sample = {'image': torch.from_numpy(img), 'keypoints': torch.from_numpy(kp)}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191371ac",
   "metadata": {},
   "source": [
    "## 訓練迴圈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895d8756",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbd4e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fdf909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from models.unet import UNet\n",
    "# from models.yolo_cnn import EnhancedYoloKeypointNet\n",
    "from models.yolo_vit import HybridKeypointNet\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "# optimization\n",
    "from torch.amp import autocast, GradScaler\n",
    "torch.backends.cudnn.benchmark = True\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Suppose images_arr: (n, 400, 400, 3), keypoints_arr: (n, 4, 2)\n",
    "dataset = KeypointDataset(img_arr, keypoints_img_arr)\n",
    "\n",
    "# split dataset into train and test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "trainloader = DataLoader(train_dataset, batch_size=8, shuffle=True, pin_memory=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=8, shuffle=False, pin_memory=True)\n",
    "\n",
    "# loss function\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(100.0))\n",
    "def pairwise_distances(pred, gt):\n",
    "    # pred: (B, K, 2), gt: (B, K, 2)\n",
    "    # returns: (B, K, K), where d[b,i,j]=||pred[b,i]-gt[b,j]||\n",
    "    diff = pred.unsqueeze(2) - gt.unsqueeze(1)  # (B, K, K, 2)\n",
    "    dist = torch.norm(diff, dim=-1)     # (B, K, K)\n",
    "    return dist\n",
    "\n",
    "def unordered_keypoint_loss(pred, gt):\n",
    "    # pred, gt: (B, K, 2)\n",
    "    dist_matrix = pairwise_distances(pred, gt)  # (B, K, K)\n",
    "    gt_to_pred = dist_matrix.min(dim=1)[0].mean(dim=1)  # mean over gt points\n",
    "    pred_to_gt = dist_matrix.min(dim=2)[0].mean(dim=1)  # mean over pred points\n",
    "    return 0.5 * (gt_to_pred + pred_to_gt).mean()  # final scalar loss\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def greedy_keypoint_assignment_loss(pred, gt):\n",
    "    # pred, gt: (B, K, 2)\n",
    "    batch_size, K, _ = pred.shape\n",
    "    losses = []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        dmat = torch.cdist(pred[b], gt[b], p=2)  # (K, K)\n",
    "        avail_pred = torch.ones(K, dtype=torch.bool, device=pred.device)\n",
    "        avail_gt = torch.ones(K, dtype=torch.bool, device=gt.device)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for _ in range(K):\n",
    "            mask = avail_pred[:, None] & avail_gt[None, :]  # (K, K)\n",
    "            masked_dmat = dmat.masked_fill(~mask, float('inf'))\n",
    "            # Flatten and find the min\n",
    "            flat_val, flat_idx = masked_dmat.view(-1).min(0)\n",
    "            i, j = divmod(flat_idx.item(), K)\n",
    "            total_loss += F.smooth_l1_loss(pred[b, i], gt[b, j], reduction='sum')\n",
    "            avail_pred[i] = False\n",
    "            avail_gt[j] = False\n",
    "        losses.append(total_loss / K)\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "# create device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# yolo cnn\n",
    "# yolo11 = YOLO('yolo11l-seg.pt')  # Or yolo11m-seg.pt, yolo11x-seg.pt, etc.\n",
    "# backbone_seq = yolo11.model.model[:12]\n",
    "# backbone = YoloBackbone(backbone_seq, selected_indices=[0,1,2,3,4,5,6,7,8,9,10,11])\n",
    "# input_dummy = torch.randn(1, 3, 512, 512)\n",
    "# with torch.no_grad():\n",
    "#     feats = backbone(input_dummy)\n",
    "# print(\"Feature shapes:\", [f.shape for f in feats])\n",
    "# in_channels_list = [f.shape[1] for f in feats]\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# keypoint_net = EnhancedYoloKeypointNet(backbone, in_channels_list)\n",
    "# model = keypoint_net\n",
    "# for param in model.backbone.parameters():\n",
    "#     param.requires_grad = False\n",
    "# model = model.to(device)\n",
    "\n",
    "# yolo vit\n",
    "yolo11 = YOLO('yolo11l-seg.pt')  # Or yolo11m-seg.pt, yolo11x-seg.pt, etc.\n",
    "backbone_seq = yolo11.model.model[:12]\n",
    "backbone = YoloBackbone(backbone_seq, selected_indices=[0,1,2,3,4,5,6,7,8,9,10,11])\n",
    "input_dummy = torch.randn(1, 3, 128, 128)\n",
    "with torch.no_grad():\n",
    "    feats = backbone(input_dummy)\n",
    "in_channels_list = [f.shape[1] for f in feats]\n",
    "keypoint_net = HybridKeypointNet(backbone, in_channels_list)\n",
    "model = keypoint_net\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "# for param in model.diffusion.vit.parameters():\n",
    "#     param.requires_grad = False\n",
    "model = model.to(device)\n",
    "\n",
    "# unet\n",
    "# model = UNet(in_channels=3, out_channels=4).to(device)\n",
    "\n",
    "compiled_model = torch.compile(model)\n",
    "if not load_model:\n",
    "    optimizer = optim.AdamW(compiled_model.parameters(), lr=1e-5)\n",
    "\n",
    "    for epoch in range(300):\n",
    "        time_start = time.time()\n",
    "        compiled_model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for batch in trainloader:\n",
    "            images = batch[\"image\"].to(device)\n",
    "            keypoints = batch[\"keypoints\"].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(\"cuda\", dtype=torch.float16):      # AMP context, not forcing .half()\n",
    "                outputs = compiled_model(images)\n",
    "                coords = soft_argmax(outputs)\n",
    "                loss = greedy_keypoint_assignment_loss(coords, keypoints)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        print(f'Epoch {epoch+1}: Loss {running_loss / len(dataset):.4f} time seconds:, {time.time() - time_start}')\n",
    "\n",
    "    # save the model\n",
    "    torch.save(compiled_model.state_dict(), 'models/keypoint_model_vit.pth')\n",
    "else:\n",
    "    \n",
    "    compiled_model.load_state_dict(torch.load('models/keypoint_mode_vit.pth', map_location=device))\n",
    "    compiled_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865d95f3",
   "metadata": {},
   "source": [
    "## 模型結果分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a512d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the validation set\n",
    "compiled_model.eval()\n",
    "val_loss = 0.0\n",
    "iter = 0\n",
    "with torch.no_grad():\n",
    "    for batch in testloader:\n",
    "        images = batch['image'].to(device)\n",
    "        keypoints = batch['keypoints'].to(device)\n",
    "        with autocast(\"cuda\", dtype=torch.float16): \n",
    "            outputs = compiled_model(images)\n",
    "        coords = soft_argmax(outputs)\n",
    "        # render the predicted keypoints on the image\n",
    "        for img, kp in zip(images.cpu().numpy(), coords.cpu().numpy()):\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "            # Convert RGB to BGR for OpenCV\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "            for i in range(kp.shape[0]):\n",
    "                cv2.circle(img, (int(kp[i][0]), int(kp[i][1])), 1, (0,0,255), -1)\n",
    "            # 撰寫測試集結果於 output 資料集\n",
    "            cv2.imwrite(f'results/keypoints_{iter}.png', img)\n",
    "            iter += 1\n",
    "        loss = greedy_keypoint_assignment_loss(coords, keypoints)\n",
    "        val_loss += loss.item() * images.size(0)\n",
    "    print(f'Validation Loss: {val_loss / len(test_dataset):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b5402",
   "metadata": {},
   "source": [
    "## 模型視覺化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0851c456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchview import draw_graph\n",
    "\n",
    "# Suppose `model` is your nn.Module, and x is a sample input tensor\n",
    "model_graph = draw_graph(model, input_data=torch.randn((8,3,128,128)), expand_nested=True)\n",
    "model_graph.visual_graph.render(filename='architecture_full', format='png')\n",
    "# or to view inline in a Jupyter notebook:\n",
    "display(model_graph.visual_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e603aaed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
