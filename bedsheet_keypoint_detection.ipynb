{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a48144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from models.yolo_vit import HybridKeypointNet\n",
    "from models.utils import *\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# yolo vit\n",
    "yolo11 = YOLO('yolo11l-seg.pt')  # Or yolo11m-seg.pt, yolo11x-seg.pt, etc.\n",
    "backbone_seq = yolo11.model.model[:12]\n",
    "backbone = YoloBackbone(backbone_seq, selected_indices=[0,1,2,3,4,5,6,7,8,9,10,11])\n",
    "input_dummy = torch.randn(1, 3, 128, 128)\n",
    "with torch.no_grad():\n",
    "    feats = backbone(input_dummy)\n",
    "in_channels_list = [f.shape[1] for f in feats]\n",
    "keypoint_net = HybridKeypointNet(backbone, in_channels_list)\n",
    "model = keypoint_net\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "# for param in model.diffusion.vit.parameters():\n",
    "#     param.requires_grad = False\n",
    "model = model.to(device)\n",
    "compiled_model = torch.compile(model)\n",
    "compiled_model.load_state_dict(torch.load('models/keypoint_model_vit.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be822ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_files = os.listdir(\"results-bed-images\")\n",
    "for img_file in img_files:\n",
    "    if img_file.endswith('.jpg'):\n",
    "        image_path = os.path.join(\"results-bed-images\", img_file)\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img / 255\n",
    "        img = cv2.resize(img, (128, 128))  # Resize to match model\n",
    "        images = torch.Tensor([np.transpose(img, (2, 0, 1))]).to(device)\n",
    "        outputs = compiled_model(images)\n",
    "        coords = soft_argmax(outputs)\n",
    "\n",
    "        # render the predicted keypoints on the image\n",
    "        for img, kp in zip(images.cpu().numpy(), coords.cpu().detach().numpy()):\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "            # Convert RGB to BGR for OpenCV\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) * 255\n",
    "            for i in range(kp.shape[0]):\n",
    "                cv2.circle(img, (int(kp[i][0]), int(kp[i][1])), 1, (0,0,255), -1)\n",
    "        cv2.imwrite(os.path.join(\"predicted-bed-keypoints\", img_file), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c04556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2733ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ffa9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
