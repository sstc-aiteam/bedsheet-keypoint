{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba71d1d1",
   "metadata": {},
   "source": [
    "# 關鍵點偵測模型訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3773dc9e",
   "metadata": {},
   "source": [
    "## 載入資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d056b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from shared.functions import *\n",
    "\n",
    "keypoints_data_src = \"via_proj/via_project_13Aug2025_15h48m06s.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ff1c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load your VIA project JSON file content (as a string or from a file)\n",
    "with open(keypoints_data_src, 'r') as f:\n",
    "    via_data = json.load(f)\n",
    "\n",
    "# Mapping from file IDs to filenames\n",
    "file_id_to_name = {fid: fdata.get('fname', '') for fid, fdata in via_data['file'].items()}\n",
    "\n",
    "# Function to get keypoints for a specified image filename\n",
    "def get_keypoints_for_image(filename):\n",
    "    # Find associated file ID for the given filename\n",
    "    file_ids = [fid for fid, name in file_id_to_name.items() if name == filename]\n",
    "    if not file_ids:\n",
    "        return None  # No matching file found\n",
    "    keypoints = []\n",
    "    for meta in via_data['metadata'].values():\n",
    "        if meta['vid'] == file_ids[0]:\n",
    "            xy = meta.get('xy', [])\n",
    "            if len(xy) == 3:\n",
    "                # xy format: [1, x, y] — take x, y\n",
    "                keypoints.append((xy[1], xy[2]))\n",
    "    return keypoints\n",
    "\n",
    "def get_image(filename):\n",
    "    img = cv2.imread(filename)\n",
    "    return img\n",
    "\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "# If using cv2/image libraries, you can import cv2 as well for actual resizing.\n",
    "\n",
    "def resize_image_and_keypoints(\n",
    "    image: np.ndarray,\n",
    "    keypoints: List[Tuple[float, float]],\n",
    "    new_width: int,\n",
    "    new_height: int\n",
    ") -> Tuple[np.ndarray, List[Tuple[float, float]]]:\n",
    "    \"\"\"\n",
    "    Resize input image and update keypoints to match the new dimensions.\n",
    "    \n",
    "    Args:\n",
    "        image (np.ndarray): Original image array.\n",
    "        keypoints (list of (x, y)): List of keypoints (floats).\n",
    "        new_width (int): Target image width.\n",
    "        new_height (int): Target image height.\n",
    "\n",
    "    Returns:\n",
    "        resized_image (np.ndarray): The resized image array.\n",
    "        resized_keypoints (list of (x, y)): Scaled-updated keypoints.\n",
    "    \"\"\"\n",
    "    orig_height, orig_width = image.shape[:2]\n",
    "    scale_x = new_width / orig_width\n",
    "    scale_y = new_height / orig_height\n",
    "\n",
    "    # If OpenCV is available:\n",
    "    # import cv2\n",
    "    resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Rescale keypoints\n",
    "    resized_keypoints = [(x * scale_x, y * scale_y) for (x, y) in keypoints]\n",
    "    return resized_image, resized_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7fcdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# 模型與圖片路徑\n",
    "model_path = \"models/yolo_finetuned/best.pt\"\n",
    "\n",
    "# 只保留這些類別 ID（根據 data.yaml 順序）\n",
    "allowed_classes = [1]  # 只要床單\n",
    "\n",
    "# 載入模型\n",
    "yolo_model_finetuned = YOLO(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccaa957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mask_compare(image_path):\n",
    "    image_name = os.path.basename(image_path)\n",
    "    # 推論\n",
    "    results = yolo_model_finetuned(image_path, task=\"segment\")[0]\n",
    "\n",
    "    # 原圖\n",
    "    orig_img = cv2.imread(image_path)\n",
    "    h, w = orig_img.shape[:2]\n",
    "\n",
    "    # 空白遮罩\n",
    "    mask_all = np.zeros((h, w), dtype=np.uint8)\n",
    "    for r in results:\n",
    "        if r.masks is None:\n",
    "            continue\n",
    "        masks = r.masks.data.cpu().numpy()     # [N, H_pred, W_pred]\n",
    "        classes = r.boxes.cls.cpu().numpy()    # [N] 物件的類別 ID\n",
    "        for m, cls_id in zip(masks, classes):\n",
    "            if int(cls_id) not in allowed_classes:\n",
    "                continue  # 跳過不在清單內的類別\n",
    "            m = (m * 255).astype(np.uint8)\n",
    "            m = cv2.resize(m, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "            mask_all = cv2.bitwise_or(mask_all, m)\n",
    "    masked_image = orig_img.copy()\n",
    "    masked_image[mask_all==0] = 0\n",
    "    return mask_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97f1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "img_arr = []\n",
    "keypoints_img_arr = []\n",
    "rgb_img_arr = []\n",
    "rgb_img_orig_arr = []\n",
    "realsense_path = \"realsense/realsense_data/realsense_camera/\"\n",
    "realsense_depth_path = \"realsense/realsense_data/depth_scaled/\"\n",
    "orig_hw = None\n",
    "for f in os.listdir(realsense_path):\n",
    "    if f[:6] == \"color_\":\n",
    "        fnumber = f[6:]; fnumber = fnumber[:-4]\n",
    "        depth_f = \"depth_raw_\" + fnumber + \".npy\"\n",
    "        color_f = \"color_\" + fnumber + \".png\"\n",
    "        depth_color_f = \"depth_color_\" + fnumber + \".png\"\n",
    "        # Example usage:\n",
    "        depth_map = np.load(realsense_depth_path + depth_f)\n",
    "        img = depth_map_to_image(depth_map)\n",
    "        img =  cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "        # Now you can save with cv2.imwrite or display with OpenCV/Matplotlib\n",
    "        color_img = get_image(realsense_path+color_f)\n",
    "        orig_hw = color_img.shape[:2]\n",
    "        # img = get_image(\"realsense_camera/\"+depth_color_f)\n",
    "        mask = extract_mask_compare(realsense_path+color_f)\n",
    "\n",
    "        # only use example where mask is detected.\n",
    "        if np.sum(mask) > 0:\n",
    "            masked_color_img = color_img.copy()\n",
    "            masked_color_img[mask==0] = 0\n",
    "            # mask the depth image\n",
    "            img[mask==0] = 0\n",
    "            orig_keypoints = get_keypoints_for_image(color_f)\n",
    "            rgb_img_orig_arr.append(color_img)\n",
    "\n",
    "            img, keypoints = resize_image_and_keypoints(img, orig_keypoints, 128, 128)\n",
    "            color_img, keypoints = resize_image_and_keypoints(color_img, orig_keypoints, 128, 128)\n",
    "            keypoints = [[kp[1], kp[0]] for kp in keypoints]\n",
    "            #kp image\n",
    "            kp_img = np.zeros((128, 128))\n",
    "            for kp in keypoints:\n",
    "                kp_img[int(kp[0]), int(kp[1])] = 1\n",
    "\n",
    "            img_arr.append(img)\n",
    "            rgb_img_arr.append(color_img)\n",
    "            keypoints_img_arr.append(kp_img)\n",
    "img_arr = np.array(img_arr)\n",
    "rgb_img_arr = np.array(rgb_img_arr)\n",
    "rgb_img_orig_arr = np.array(rgb_img_orig_arr)\n",
    "keypoints_img_arr = np.array(keypoints_img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a4188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "\n",
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, images, rgb_images, rgb_origs, keypoints, transform=None):\n",
    "        self.images = images.astype(np.float32)\n",
    "        self.rgb_images = rgb_images.astype(np.float32)\n",
    "        self.rgb_orig = rgb_origs.astype(np.float32)\n",
    "        self.keypoints = keypoints.astype(np.float32)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        rgb_img = self.rgb_images[idx]\n",
    "        kp = self.keypoints[idx]  # shape (4, 2)\n",
    "        img = np.transpose(img, (2, 0, 1))  # channels first\n",
    "        rgb_img = np.transpose(rgb_img, (2, 0, 1))\n",
    "        rgb_orig = self.rgb_orig[idx]\n",
    "        sample = {'image': torch.from_numpy(img), 'keypoints': torch.from_numpy(kp), 'rgb_image': torch.from_numpy(rgb_img), \"rgb_orig\": torch.from_numpy(rgb_orig)}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "\n",
    "class RandomRotateFlip:\n",
    "    \"\"\"\n",
    "    Randomly applies:\n",
    "    - A rotation by any angle in [0, 360)\n",
    "    - Optionally, a horizontal flip with 50% chance after rotation\n",
    "    \"\"\"\n",
    "    def __call__(self, sample):\n",
    "        image, rgb_image, keypoints, rgb_orig = sample['image'], sample[\"rgb_image\"], sample['keypoints'], sample[\"rgb_orig\"]\n",
    "        # image: (C, H, W)\n",
    "        # keypoints: (N, H, W) or (H, W)\n",
    "\n",
    "        # --- Random rotation ---\n",
    "        angle = random.uniform(0, 360)\n",
    "        image = TF.rotate(image, angle, interpolation=TF.InterpolationMode.BILINEAR)\n",
    "        rgb_image = TF.rotate(rgb_image, angle, interpolation=TF.InterpolationMode.BILINEAR)\n",
    "        # For keypoints as heatmaps, use same rotate (assume keypoints is Tensor [N,H,W] or [H,W])\n",
    "        # If N, treat each as a channel\n",
    "        if keypoints.ndim == 3:\n",
    "            keypoints = TF.rotate(keypoints, angle, interpolation=TF.InterpolationMode.BILINEAR)\n",
    "        else:\n",
    "            keypoints = TF.rotate(keypoints.unsqueeze(0), angle, interpolation=TF.InterpolationMode.BILINEAR).squeeze(0)\n",
    "\n",
    "        # --- Random flip after rotation ---\n",
    "        if random.random() < 0.5:\n",
    "            image = TF.hflip(image)\n",
    "            rgb_image = TF.hflip(rgb_image)\n",
    "            keypoints = TF.hflip(keypoints)\n",
    "        if random.random() < 0.5:\n",
    "            image = TF.vflip(image)\n",
    "            rgb_image = TF.vflip(rgb_image)\n",
    "            keypoints = TF.vflip(keypoints)\n",
    "\n",
    "        return {'image': image, 'rgb_image': rgb_image, 'keypoints': keypoints, \"rgb_orig\": rgb_orig}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f608ce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotate_transform = RandomRotateFlip()\n",
    "\n",
    "# Create the full dataset without transform\n",
    "full_dataset = KeypointDataset(img_arr, rgb_img_arr, rgb_img_orig_arr, keypoints_img_arr, transform=None)\n",
    "\n",
    "# Split indices for train/test\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_indices, test_indices = torch.utils.data.random_split(range(len(full_dataset)), [train_size, test_size])\n",
    "\n",
    "# Create train and test datasets with/without transform\n",
    "train_dataset = torch.utils.data.Subset(KeypointDataset(img_arr, rgb_img_arr, rgb_img_orig_arr,keypoints_img_arr, transform=rotate_transform), train_indices)\n",
    "test_dataset = torch.utils.data.Subset(KeypointDataset(img_arr, rgb_img_arr, rgb_img_orig_arr,keypoints_img_arr, transform=None), test_indices)\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=8, shuffle=True, pin_memory=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=8, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ac2257",
   "metadata": {},
   "source": [
    "## 測試關鍵點跟圖片的正確性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6febeefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = full_dataset.__getitem__(9)\n",
    "img = pair[\"image\"].numpy().copy() / 255\n",
    "img = np.transpose(img, (1, 2, 0))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "rgb_img = pair[\"rgb_image\"].numpy().copy() / 255\n",
    "rgb_img = np.transpose(rgb_img, (1, 2, 0))\n",
    "rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "kp = pair[\"keypoints\"].numpy()\n",
    "print(img.shape, kp.shape)\n",
    "for i in range(kp.shape[0]):\n",
    "    for j in range(kp.shape[1]):\n",
    "        if kp[i,j] > 0.1:\n",
    "            cv2.circle(img, (j, i), 1, (0,0,255), -1)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "plt.imshow(rgb_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191371ac",
   "metadata": {},
   "source": [
    "## 訓練迴圈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895d8756",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbd4e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e6bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_klloss(pred_map, target_map, eps=1e-8):\n",
    "    # pred_map: after spatial softmax, (B, 1, H, W)\n",
    "    # target_map: one-hot or few-hot, (B, H, W)\n",
    "    B, _, H, W = pred_map.shape\n",
    "    pred = pred_map.view(B, -1) + eps  # avoid log(0)\n",
    "    target = target_map.view(B, -1) + eps\n",
    "    pred_log = pred.log()\n",
    "    target = target / target.sum(dim=1, keepdim=True)  # ensure sum-to-1; safe for multi-keypoint\n",
    "    return (target * (target.log() - pred_log)).sum(dim=1).mean()\n",
    "\n",
    "def kl_heatmap_loss(pred_hm, gt_hm, mask=None, reduction='mean'):\n",
    "    \"\"\"\n",
    "    pred_hm: (B, 1, H, W) tensor, model output (must be positive, not all zeros)\n",
    "    gt_hm:   (B, 1, H, W) tensor, ground-truth (should be positive, not all zeros)\n",
    "    mask:    (B, 1, H, W) optional mask (1=valid, 0=ignored) or None\n",
    "    reduction: 'mean', 'sum', or 'none'\n",
    "    Returns: scalar loss\n",
    "    \"\"\"\n",
    "    B, _, H, W = pred_hm.shape\n",
    "    # Flatten\n",
    "    pred_flat = pred_hm.view(B, -1)\n",
    "    gt_flat = gt_hm.view(B, -1)\n",
    "\n",
    "    # Force positive and normalize to sum=1 for both (prob dists)\n",
    "    pred_probs = pred_flat.clamp(min=1e-8)\n",
    "    pred_probs = pred_probs / pred_probs.sum(dim=1, keepdim=True)\n",
    "    gt_probs = gt_flat.clamp(min=1e-8)\n",
    "    gt_probs = gt_probs / gt_probs.sum(dim=1, keepdim=True)\n",
    "\n",
    "    kl_div = F.kl_div(pred_probs.log(), gt_probs, reduction='none').sum(dim=1)  # KL per sample\n",
    "\n",
    "    if reduction == 'mean':\n",
    "        return kl_div.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return kl_div.sum()\n",
    "    else:\n",
    "        return kl_div  # shape (B,)\n",
    "    \n",
    "def batch_gaussian_blur(x, kernel_size=5, sigma=2.0):\n",
    "    \"\"\"\n",
    "    Apply Gaussian blur to a batch of heatmaps and normalize each so the max is 1.\n",
    "    Args:\n",
    "        x: Tensor [B, H, W] or [B, 1, H, W]\n",
    "    Returns:\n",
    "        Tensor with same shape, blurred and with peak 1 per sample\n",
    "    \"\"\"\n",
    "    unsqueeze = False\n",
    "    if x.dim() == 3:  # [B, H, W]\n",
    "        x = x.unsqueeze(1)\n",
    "        unsqueeze = True\n",
    "    \n",
    "    blurred = TF.gaussian_blur(x, kernel_size=[kernel_size, kernel_size], sigma=[sigma, sigma])\n",
    "    max_vals = blurred.amax(dim=[2, 3], keepdim=True)\n",
    "    max_vals[max_vals == 0] = 1.0  # Avoid division by zero\n",
    "    normalized = blurred / max_vals\n",
    "\n",
    "    if unsqueeze:\n",
    "        normalized = normalized.squeeze(1)\n",
    "    return normalized\n",
    "\n",
    "def batch_entropy(pred_heatmaps):\n",
    "    \"\"\"\n",
    "    pred_heatmaps: [B, C, H, W]\n",
    "    Returns: [B] entropy per image\n",
    "    \"\"\"\n",
    "    # Flatten spatial dimensions (and optionally channels) for softmax\n",
    "    B, C, H, W = pred_heatmaps.shape\n",
    "    flat = pred_heatmaps.view(B, -1)                # [B, C*H*W]\n",
    "    probs = torch.softmax(flat, dim=1)              # normalize to sum=1 per image\n",
    "    entropy = -(probs * torch.log(probs + 1e-8)).sum(dim=1)  # [B]\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fdf909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from models.unet import UNet\n",
    "# from models.yolo_cnn import EnhancedYoloKeypointNet\n",
    "from models.yolo_vit import HybridKeypointNet\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "# optimization\n",
    "from torch.amp import autocast, GradScaler\n",
    "torch.backends.cudnn.benchmark = True\n",
    "scaler = GradScaler()\n",
    "\n",
    "# create device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# yolo cnn\n",
    "# yolo11 = YOLO('yolo11l-seg.pt')  # Or yolo11m-seg.pt, yolo11x-seg.pt, etc.\n",
    "# backbone_seq = yolo11.model.model[:12]\n",
    "# backbone = YoloBackbone(backbone_seq, selected_indices=[0,1,2,3,4,5,6,7,8,9,10,11])\n",
    "# input_dummy = torch.randn(1, 3, 512, 512)\n",
    "# with torch.no_grad():\n",
    "#     feats = backbone(input_dummy)\n",
    "# print(\"Feature shapes:\", [f.shape for f in feats])\n",
    "# in_channels_list = [f.shape[1] for f in feats]\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# keypoint_net = EnhancedYoloKeypointNet(backbone, in_channels_list)\n",
    "# model = keypoint_net\n",
    "# for param in model.backbone.parameters():\n",
    "#     param.requires_grad = False\n",
    "# model = model.to(device)\n",
    "\n",
    "# yolo vit\n",
    "yolo_model = YOLO('yolov8l.pt')\n",
    "backbone_seq = yolo_model.model.model[:10]\n",
    "backbone = YoloBackbone(backbone_seq, selected_indices=[0,1,2,3,4,5,6,7,8,9])\n",
    "input_dummy = torch.randn(1, 3, 128, 128)\n",
    "with torch.no_grad():\n",
    "    feats = backbone(input_dummy)\n",
    "in_channels_list = [f.shape[1] for f in feats]\n",
    "keypoint_net = HybridKeypointNet(backbone, in_channels_list)\n",
    "model = keypoint_net\n",
    "model = model.to(device)\n",
    "\n",
    "# unet\n",
    "# model = UNet(in_channels=3, out_channels=4).to(device)\n",
    "\n",
    "compiled_model = torch.compile(model)\n",
    "# load pretrained model\n",
    "compiled_model.load_state_dict(torch.load('models/keypoint_model_vit.pth', map_location=device))\n",
    "compiled_model.eval()\n",
    "\n",
    "if not load_model:\n",
    "    optimizer = optim.AdamW(compiled_model.parameters(), lr=1e-5)\n",
    "\n",
    "    for epoch in range(300):\n",
    "        time_start = time.time()\n",
    "        compiled_model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for batch in trainloader:\n",
    "            images = batch[\"image\"].to(device)\n",
    "            keypoints = batch[\"keypoints\"].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(\"cuda\", dtype=torch.float16):      # AMP context, not forcing .half()\n",
    "                outputs = compiled_model(images)\n",
    "                keypoints_blur = batch_gaussian_blur(keypoints, kernel_size=31, sigma=3)\n",
    "                \n",
    "                # active learning: Uncertainty Sampling using entropy as the uncertainty metric\n",
    "                entropies = batch_entropy(outputs)\n",
    "                k = images.size(0) // 2\n",
    "                topk_vals, topk_idx = torch.topk(entropies, k, largest=True)  # highest entropy first\n",
    "                selected_outputs = outputs[topk_idx]\n",
    "                selected_keypoints_blur = keypoints_blur[topk_idx]\n",
    "\n",
    "                # calculate loss\n",
    "                loss = kl_heatmap_loss(selected_outputs, selected_keypoints_blur.unsqueeze(1))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        print(f'Epoch {epoch+1}: Loss {running_loss / len(train_dataset):.4f} time seconds:, {time.time() - time_start}')\n",
    "\n",
    "    # save the model\n",
    "    torch.save(compiled_model.state_dict(), 'models/keypoint_model_vit_depth.pth')\n",
    "else:\n",
    "    \n",
    "    compiled_model.load_state_dict(torch.load('models/keypoint_model_vit_depth.pth', map_location=device))\n",
    "    compiled_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865d95f3",
   "metadata": {},
   "source": [
    "## 模型結果分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a512d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the validation set\n",
    "compiled_model.eval()\n",
    "val_loss = 0.0\n",
    "iter = 0\n",
    "with torch.no_grad():\n",
    "    for batch in testloader:\n",
    "        images = batch['image'].to(device)\n",
    "        rgb_images = batch['rgb_image'].to(device)\n",
    "        keypoints = batch['keypoints'].to(device)\n",
    "        rgb_origs = batch['rgb_orig'].to(device)\n",
    "        with autocast(\"cuda\", dtype=torch.float16): \n",
    "            outputs = compiled_model(images)\n",
    "            keypoints_blur = batch_gaussian_blur(keypoints, kernel_size=31, sigma=3)\n",
    "            loss = kl_heatmap_loss(outputs, keypoints_blur.unsqueeze(1))\n",
    "        # render the predicted keypoints on the image\n",
    "        for img, kp, rgb_img, rgb_orig in zip(images.cpu().numpy(), outputs.cpu().numpy(), rgb_images.cpu().numpy(), rgb_origs.cpu().numpy()):\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "            rgb_img = rgb_orig\n",
    "            # Convert RGB to BGR for OpenCV\n",
    "            rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_RGB2BGR)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "            kp = kp[0,:,:]\n",
    "            peaks = thresholded_locations(kp, 0.003)\n",
    "            for p in peaks:\n",
    "                i,j = p\n",
    "                cv2.circle(rgb_img, (int(j * orig_hw[1]/128), int(i * orig_hw[0]/128)), 10, (255,0,0), -1)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)\n",
    "            # 撰寫測試集結果於 output 資料集\n",
    "            # Resize img to original size using orig_hw\n",
    "            cv2.imwrite(f'results/keypoints_{iter}.png', rgb_img)\n",
    "            iter += 1\n",
    "        val_loss += loss.item() * images.size(0)\n",
    "    print(f'Validation Loss: {val_loss / len(test_dataset):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b5402",
   "metadata": {},
   "source": [
    "## 模型視覺化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0851c456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchview import draw_graph\n",
    "\n",
    "# Suppose `model` is your nn.Module, and x is a sample input tensor\n",
    "model_graph = draw_graph(model, input_data=torch.randn((8,3,128,128)), expand_nested=True)\n",
    "model_graph.visual_graph.render(filename='architecture_full', format='png')\n",
    "# or to view inline in a Jupyter notebook:\n",
    "display(model_graph.visual_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e603aaed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
